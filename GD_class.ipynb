{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GD class.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PAJqmfbAyXuK",
        "BjhdCRvYyb_A",
        "t8ncTw7lz6Tu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "X = 2*np.random.rand(100,1)\n",
        "X_ = np.c_[np.ones((100,1)),X]\n",
        "y = 4*X + 2 + 2*np.random.rand()-0.5"
      ],
      "metadata": {
        "id": "nfZcweDhgtlJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.function_base import gradient\n",
        "import numpy as np\n",
        "class Linear_Regression():\n",
        "  def __init__(self, alpha, lambd, learning_rate, n_iter, eta, epochs, batch_size):\n",
        "    self.theta= np.zeros(2).reshape(2,1)\n",
        "    self.alpha= alpha\n",
        "    self.lambd= lambd\n",
        "    self.learning_rate= learning_rate\n",
        "    self.n_iter= n_iter\n",
        "    self.eta= eta\n",
        "    self.epochs= epochs\n",
        "    self.batch_size= batch_size\n",
        "  def normal_equation(self):\n",
        "    return np.matmul(np.matmul(np.linalg.inv(np.matmul(self.X.T, self.X)), self.X.T), self.Y)\n",
        "\n",
        "  def update_coeffs(self, X, Y):\n",
        "      Y_pred = self.predict(X)\n",
        "      m = len(X)\n",
        "      r= self.get_regularization()\n",
        "      self.theta = self.theta - (self.learning_rate/m) * (X.T.dot(Y_pred - Y) + r)\n",
        "    \n",
        "  def predict(self, X):\n",
        "    return X.dot(self.theta)\n",
        "\n",
        "  def batchgradientDescent(self):\n",
        "    m= len(self.X)\n",
        "    for i in range(n_iter):\n",
        "      self.update_coeffs(self.X, self.Y)\n",
        "    return self.theta\n",
        "  \n",
        "  def get_regularization(self):\n",
        "     return (self.lambd * ((1-self.alpha) * self.theta + self.alpha * np.sign(self.theta)))\n",
        "\n",
        "  def create_mini_batches(self):\n",
        "    data = np.hstack((self.X, self.Y))\n",
        "    np.random.shuffle(data)\n",
        "    size= self.batch_size\n",
        "    if(len(data)%size==0):\n",
        "      s= len(data)/size\n",
        "      batch_data= np.split(data, s)\n",
        "    else:\n",
        "      m= len(data)%size\n",
        "      s= (len(data)-m)/size\n",
        "      batch_data= np.split(data, s)\n",
        "      batch_data.append(data[:-m])\n",
        "    return batch_data\n",
        "\n",
        "  def minibatchgradient_descent(self):\n",
        "    m= self.batch_size\n",
        "    for i in range(self.epochs):\n",
        "      batch= self.create_mini_batches()\n",
        "      for mini_batch in batch:\n",
        "        X_mini= mini_batch[:,:-1]\n",
        "        y_mini= mini_batch[:,-1].reshape(m, 1)\n",
        "        self.update_coeffs(X_mini, y_mini)\n",
        "    return self.theta\n",
        "\n",
        "  def sgd(self):\n",
        "    m= len(self.X)\n",
        "    data= np.concatenate((self.X, self.Y), axis=1)\n",
        "    for i in range(self.epochs):\n",
        "      np.random.shuffle(data)\n",
        "      tempX= data[:,:-1].reshape(100, data.shape[1]-1)\n",
        "      tempy= data[:,-1].reshape(100,1)\n",
        "      for j in range(m):\n",
        "        xtemp= tempX[j:j+1].reshape(1,data.shape[1]-1)\n",
        "        self.update_coeffs(xtemp, tempy[j:j+1])\n",
        "    return self.theta\n",
        "    \n",
        "  def fit(self, X, y):\n",
        "    X_ = np.c_[np.ones((100,1)),X]\n",
        "    self.X= X_\n",
        "    self.Y= y\n",
        "\n",
        "\n",
        "alpha= 0.0001\n",
        "lambd=0.001\n",
        "learning_rate=0.1\n",
        "n_iter=500\n",
        "eta=0.1\n",
        "epochs= 50\n",
        "batch_size= 20\n",
        "reg= Linear_Regression(alpha, lambd, learning_rate, n_iter, eta, epochs, batch_size)\n",
        "reg.fit(X, y)\n",
        "r= reg.batchgradientDescent()\n",
        "print(\"Pred: \", r[0], r[1])\n",
        "from sklearn import linear_model\n",
        "elreg = linear_model.ElasticNet(alpha = 0.001, l1_ratio= 0.0001, max_iter=500).fit(X, y)\n",
        "print(\"Actual: \", elreg.intercept_, elreg.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jeOackRvbhk",
        "outputId": "e7651a5f-0693-46a2-e916-d9955aea2385"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pred:  [3.10387063] [3.99982766]\n",
            "Actual:  [3.11582694] [3.98691018]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qIpG26gcT0n3"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}